# ==== Broker Connection ==== #
[connection]
brokers = ["localhost:9092"]
use_ssl = false

[connection.ssl]
ca_file = "/path/to/ca.pem"
client_cert_file = "/path/to/client_certificate.pem"
client_key_file = "/path/to/client_key.pem"
client_key_password = "password for the private key"
check_hostname = true

# ==== Low-Level Client Options ==== #
[consumer]
# Options passed directly to confluent-kafka and its underlying librdkafka library.
# The dotted keys must be enclosed in "".
# See: https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md
"session.timeout.ms" = 10000
"auto.commit.interval.ms" = 3000
"broker.address.family" = "v4"

[producer]
# Same as above.
"compression.codec" = "zstd"
"linger.ms" = 1000
"batch.num.messages" = 10000
"request.required.acks" = "all"
"message.send.max.retries" = 100
"broker.address.family" = "v4"

# ==== Client Options ==== #
[client]
app_id = "domrad-extractor"
mp_start_method = "forkserver"
# Number of worker processes
workers = 4
# How long to wait in a single Kafka consume request
poll_timeout = 2.0
# How long to wait for a worker to finish (seconds)
worker_kill_timeout = 5.0
# How often check if all worker processes live (seconds)
liveliness_check_interval = 10.0
# How long to wait for the worker processes to initialize before starting to consume input requests (seconds)
init_wait = 1.0
# How long an entry can wait in the to-process queue before a warning is logged (seconds)
entry_late_warning_threshold = 15
# How long an entry can wait in the to-process queue before the whole system gives up (seconds)
max_entry_time_in_queue = 100
# The maximum input queue size
max_queued_items = 10
# After reaching the maximum input queue size specified above, this number of input entries must be processed
# and produced before new items are added to the queue
resume_after_freed_items = 2
# The maximum number of items to retrieve from the processed queue in a single cycle (livelock prevention threshold)
max_entries_to_confirm = 10

# ==== Extractor-Specific Options ==== #
[extractor]
data_dir = "./extractor/data/"
# Number of records to collect in a buffer and process at once.
# This should be fairly high, according to the input rate.
# The downside of batching is that in case of an unhandled exception,
# the whole batch is lost.
batch_size = 100
# Maximum time (in seconds) to wait for a full batch before processing.
# In other words, the extractor will always process its current buffer
# `batch_timeout` seconds after the last processed one, even if it's not full.
batch_timeout = 5
# This property controls the applied transformations. The order is irelevant, it is set by the code.
# You probably don't want to change this. Leave the property out to use all available transformations.
enabled_transformations = ["html", "dns", "ip", "geo", "tls", "lexical", "rdap_dn", "rdap_ip", "drop"]
# When set to true, the feature extractor will produce the JSON-serialized feature vectors one-by-one to the
# feature_vectors_json topic. This is useful for debugging and testing the feature extractor, as well as for
# the "standalone" mode, where the extractor is not connected to the rest of the pipeline.
produce_jsons = true
# When set to true, the feature extractor won't produce the binary-serialised dataframes to the feature_vectors topic.
only_produce_jsons = false

# ==== Logging ==== #
[logging]
version = 1
disable_existing_loggers = true
worker_level = "TRACE"

[logging.formatters.simple]
class = "logging.Formatter"
format = "[%(name)s][%(process)s|%(processName)s][%(levelname)s]\t%(asctime)s: %(message)s"

[logging.formatters.extra]
class = "common.log.ExtraFormatter"

[logging.handlers.console]
class = "logging.StreamHandler"
formatter = "simple"
level = "TRACE"
stream = "ext://sys.stdout"

[logging.handlers.err]
class = "logging.StreamHandler"
formatter = "simple"
level = "WARNING"
stream = "ext://sys.stderr"

[logging.root]
handlers = ["console", "err"]
level = "INFO"

# ==== Metrics ==== #
[metrics]
enabled = true

[metrics.server]
# Options passed as kwargs for prometheus_client.start_http_server
port = 8080
addr = "127.0.0.1"

